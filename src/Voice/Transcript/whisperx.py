import os
import subprocess
import glob
import json
import logging
from datetime import datetime
from tqdm import tqdm
import time

def transcribe_audio(
    file_path: str,
    model: str = "large-v2",
    align_model: str | None = None,
    batch_size: int | None = None,
    compute_type: str | None = None,
    language: str | None = None,
    diarize: bool = False,
    highlight_words: bool = False,
    min_speakers: int | None = None,
    max_speakers: int | None = None,
    hf_token: str | None = None,
    extra_args: dict | None = None,
    output_dir: str | None = None
):
    """
    Transcribe audio using WhisperX CLI with flexible options.

    Parameters:
        file_path (str): 
            Path to the input audio file to be transcribed. Supported formats include wav, mp3, etc.

        model (str, default="large-v2"): 
            The Whisper ASR model to use for transcription.
            Options include "base", "small", "medium", "large", "large-v2", etc.
            Larger models are more accurate but require more memory and computation.

        align_model (str, optional):
            The phoneme-based forced alignment model used to precisely align the transcript to the audio, producing word-level timestamps.
            Alignment in WhisperX works by matching the output of the ASR model (transcribed text) to the audio signal using a specialized model 
            (often a wav2vec2 variant). This process assigns accurate timings to each word or phoneme.
            Forced alignment leverages models trained to recognize small units of speech (phonemes), 
            mapping text segments to corresponding audio sections. This is particularly useful for generating subtitles or timestamped transcripts.
            Common choices include "WAV2VEC2_ASR_LARGE_LV60K_960H" for English. 
            For other languages, WhisperX will attempt to select an appropriate alignment model automatically if not specified.
            For more details, see the WhisperX paper: "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio" (Bain et al., INTERSPEECH 2023)

        batch_size (int, optional): 
            Number of audio segments to process in parallel during inference.
            Increasing batch size speeds up transcription but uses more GPU memory.
            Lower batch size may help on machines with less available memory.

        compute_type (str, optional): 
            Precision type for model inference.
            Common values include "float16" (for faster inference on GPUs with good accuracy),
            and "int8" (for lower memory usage, but may reduce accuracy).
            Use "int8" if you are low on GPU memory or running on CPU.

        language (str, optional): 
            Language code of the audio (e.g., "en" for English, "de" for German).
            This selects the appropriate phoneme alignment model for better timestamp accuracy.
            Useful for multilingual transcription.

        diarize (bool, default=False): 
            If True, enables speaker diarization, partitioning the transcript by speaker identity.
            Requires a HuggingFace access token (see `hf_token`).
            Useful for multi-speaker recordings.

        highlight_words (bool, default=False): 
            If True, highlights word-level timings in the output subtitle (.srt) file.
            Helps visualize precise word alignment in subtitles.

        min_speakers (int, optional): 
            Minimum number of speakers to expect in speaker diarization.
            If known, setting this improves diarization accuracy.

        max_speakers (int, optional): 
            Maximum number of speakers to expect in speaker diarization.
            If known, setting this improves diarization accuracy.

        hf_token (str, optional): 
            HuggingFace access token for downloading speaker diarization models.
            Required if `diarize=True`.

        extra_args (list of str, optional): 
            Additional command-line arguments to pass to WhisperX.
            Useful for advanced usage not explicitly covered by other parameters.
            
        output_dir (str, optional):
            Directory to save output files. If not provided, uses same directory as input file.

    Returns:
        dict: Dictionary containing paths to output files generated by WhisperX
    """
    cmd = ["uvx", "whisperx", file_path]
    cmd += ["--model", model]
    
    if align_model:
        cmd += ["--align_model", align_model]
    if batch_size:
        cmd += ["--batch_size", str(batch_size)]
    if compute_type:
        cmd += ["--compute_type", compute_type]
    if language:
        cmd += ["--language", language]
    if diarize:
        cmd += ["--diarize"]
    if highlight_words:
        cmd += ["--highlight_words", "True"]
    if min_speakers:
        cmd += ["--min_speakers", str(min_speakers)]
    if max_speakers:
        cmd += ["--max_speakers", str(max_speakers)]
    if hf_token:
        cmd += ["--hf_token", hf_token]
    if output_dir:
        cmd += ["--output_dir", output_dir]
    if extra_args:
        cmd += extra_args
    
    start_time = time.time()
    process = subprocess.run(cmd, capture_output=True, text=True, check=False)
    elapsed_time = time.time() - start_time
    
    # Get base filename without extension
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # Determine output directory
    output_directory = output_dir if output_dir else os.path.dirname(file_path)
    
    # Expected output files
    expected_outputs = {
        "json": os.path.join(output_directory, f"{base_name}.json"),
        "srt": os.path.join(output_directory, f"{base_name}.srt"),
        "txt": os.path.join(output_directory, f"{base_name}.txt"),
        "vtt": os.path.join(output_directory, f"{base_name}.vtt"),
    }
    
    # Check which files were actually created
    results = {
        "success": process.returncode == 0,
        "elapsed_time": elapsed_time,
        "outputs": {k: v for k, v in expected_outputs.items() if os.path.exists(v)},
        "stdout": process.stdout,
        "stderr": process.stderr,
        "returncode": process.returncode,
    }
    
    return results

def process_audio_folder(folderpath, audio_extensions=None, output_dir=None, log_path=None, **kwargs):
    """
    Process all audio files in a folder using WhisperX transcription.
    
    Parameters:
        folderpath (str):
            Path to the folder containing audio files
        
        audio_extensions (list, optional):
            List of audio file extensions to process. Default: ['.wav', '.mp3', '.m4a', '.flac', '.ogg']
        
        output_dir (str, optional):
            Directory to save all transcription outputs. If None, saves in same directories as input files.
            
        log_path (str, optional):
            Path to save the log file. If None, saves log in the folderpath with timestamp.
            
        **kwargs:
            Additional arguments to pass to transcribe_audio function
    
    Returns:
        dict: Dictionary with audio file paths as keys and transcription results as values
    """
    # Setup logging
    if log_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_path = os.path.join(folderpath, f"transcription_log_{timestamp}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()
        ]
    )
    
    # Default audio extensions if not provided
    if audio_extensions is None:
        audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.ogg', '.mp4', '.avi', '.mov']
    
    # Find all audio files
    audio_files = []
    for ext in audio_extensions:
        audio_files.extend(glob.glob(os.path.join(folderpath, f"*{ext}")))
        audio_files.extend(glob.glob(os.path.join(folderpath, f"**/*{ext}"), recursive=True))
    
    audio_files = sorted(list(set(audio_files)))  # Remove duplicates and sort
    
    if not audio_files:
        logging.warning(f"No audio files found in {folderpath} with extensions {audio_extensions}")
        return {}
    
    logging.info(f"Found {len(audio_files)} audio files to process")
    
    # Create output directory if specified
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        logging.info(f"Created output directory: {output_dir}")
    
    # Process each file
    results = {}
    failed_files = []
    
    for audio_file in tqdm(audio_files, desc="Processing audio files"):
        file_name = os.path.basename(audio_file)
        logging.info(f"Processing file: {file_name}")
        
        try:
            result = transcribe_audio(audio_file, output_dir=output_dir, **kwargs)
            results[audio_file] = result
            
            if result["success"]:
                logging.info(f"Successfully processed {file_name} in {result['elapsed_time']:.2f} seconds")
            else:
                logging.error(f"Failed to process {file_name}. Error: {result['stderr']}")
                failed_files.append(audio_file)
                
        except Exception as e:
            logging.exception(f"Exception while processing {file_name}: {str(e)}")
            failed_files.append(audio_file)
            results[audio_file] = {"success": False, "error": str(e)}
    
    # Save summary
    summary = {
        "total_files": len(audio_files),
        "successful": len(audio_files) - len(failed_files),
        "failed": len(failed_files),
        "failed_files": failed_files
    }
    
    summary_path = os.path.join(output_dir if output_dir else folderpath, "transcription_summary.json")
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    logging.info(f"Processing completed. Summary: {summary['successful']}/{summary['total_files']} files processed successfully.")
    logging.info(f"Log saved to {log_path}")
    logging.info(f"Summary saved to {summary_path}")
    
    return results